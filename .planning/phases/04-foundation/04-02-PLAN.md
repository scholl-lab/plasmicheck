---
phase: 04-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/benchmark.py
autonomous: true

must_haves:
  truths:
    - "Running `python scripts/benchmark.py` times each pipeline step separately and outputs a Markdown table"
    - "Benchmark reports mean and standard deviation over 3 iterations"
    - "Benchmark times individual steps: convert, index, spliced, align, compare, report"
    - "Output BENCHMARK.md is readable and shows per-step timing breakdown"
    - "Benchmark supports both synthetic (default) and custom dataset paths"
  artifacts:
    - path: "scripts/benchmark.py"
      provides: "Standalone benchmark script with per-step timing"
      min_lines: 150
    - path: "BENCHMARK.md"
      provides: "Markdown benchmark results (generated, gitignored)"
  key_links:
    - from: "scripts/benchmark.py"
      to: "plasmicheck.scripts.run_pipeline.run_pipeline"
      via: "Cannot use run_pipeline directly (monolithic). Must call individual step functions for per-step timing."
      pattern: "from plasmicheck\\.scripts"
    - from: "scripts/benchmark.py"
      to: "tests/data/synthetic/"
      via: "Default dataset path for benchmark runs"
      pattern: "tests/data/synthetic"
---

<objective>
Create a standalone benchmark script that measures per-step pipeline timing and outputs a Markdown report.

Purpose: This provides the performance comparison tool needed to validate that Phases 5-7 optimizations actually improve performance. Developers will run this before and after optimization PRs to quantify speedup per pipeline step.

Output: `scripts/benchmark.py` -- a self-contained CLI script that runs the pipeline with per-step timing and generates `BENCHMARK.md`.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-foundation/04-CONTEXT.md
@.planning/phases/04-foundation/04-RESEARCH.md

Key source files:
@plasmicheck/scripts/run_pipeline.py (pipeline orchestration -- understand step sequence)
@plasmicheck/scripts/convert_plasmidfile_to_fasta.py (convert step)
@plasmicheck/scripts/create_indexes.py (index step)
@plasmicheck/scripts/spliced_alignment.py (spliced alignment step)
@plasmicheck/scripts/align_reads.py (alignment step)
@plasmicheck/scripts/compare_alignments.py (comparison step)
@plasmicheck/scripts/generate_report.py (report generation step)
@plasmicheck/version.py (version string)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create benchmark script with per-step timing</name>
  <files>scripts/benchmark.py</files>
  <action>
Create `scripts/benchmark.py` as a standalone CLI script. The key challenge is that `run_pipeline()` is monolithic -- it runs all steps internally. To time each step, the benchmark must replicate the pipeline flow using individual step functions.

**CLI interface (argparse):**
- `python scripts/benchmark.py` -- Run 3 iterations on synthetic dataset, output to BENCHMARK.md
- `python scripts/benchmark.py -n 5` -- Override iteration count
- `python scripts/benchmark.py -o results.md` -- Override output file
- `python scripts/benchmark.py --dataset-dir PATH` -- Use custom dataset directory (must contain human_ref.fasta, plasmid.gb, contaminated_R1.fastq, contaminated_R2.fastq)
- `python scripts/benchmark.py --steps convert,align,compare` -- Benchmark only specific steps (comma-separated)

**Per-step timing approach:**

Study `run_pipeline.py` to understand the exact step sequence, then replicate it with timing wrappers. The pipeline flow for a single combination is:

1. **convert**: `convert_plasmidfile_to_fasta(plasmid_gb, plasmid_fasta, "genbank")`
2. **index**: `create_indexes(human_fasta)` + `create_indexes(plasmid_fasta)`
3. **spliced**: `spliced_alignment()` + `extract_human_reference()` + `extract_plasmid_cdna_positions()`
4. **align**: `align_reads()` for both plasmid and human references
5. **compare**: `compare_alignments()`
6. **report**: `generate_report()`

Use a `timed_step` context manager (from RESEARCH.md Pattern 2):
```python
@contextmanager
def timed_step(name: str, timings: dict[str, float]) -> Generator[None, None, None]:
    start = time.perf_counter()
    try:
        yield
    finally:
        timings[name] = time.perf_counter() - start
```

**Iteration logic:**
1. Run one warm-up iteration (not counted) to avoid import/index-generation overhead skewing results
2. Run N iterations (default 3), collecting per-step timings for each
3. For each step, compute mean, stdev (using `statistics.mean()`, `statistics.stdev()`)
4. Also compute total pipeline time per iteration

**Important: Use tempfile.TemporaryDirectory for each iteration.**
Each iteration should run into a fresh temp output directory, then clean up. Do NOT reuse output directories because `--overwrite` behavior could skip steps.

However, index files (`.mmi`, `.fai`) should be pre-generated during warm-up and reused across iterations (they don't change). Copy the pre-generated indexes into each iteration's temp dir, or ensure indexes exist before timing starts (index step timing should reflect the steady-state, not first-run generation).

Actually, simpler approach: time the **full pipeline call** per-step by calling individual functions, but for the index step, time it once and note "index generation is a one-time cost." For subsequent iterations, skip the index step if indexes already exist (matching real-world usage).

**Markdown output format (BENCHMARK.md):**

```markdown
# PlasmiCheck Benchmark Results

**Version:** {version}
**Date:** {ISO date}
**Dataset:** {dataset name} ({read count} reads)
**Iterations:** {N} (+ 1 warm-up)
**System:** {platform.system()} {platform.machine()}

## Per-Step Timing

| Step | Mean (s) | Std Dev (s) | Min (s) | Max (s) | % of Total |
|------|----------|-------------|---------|---------|------------|
| convert | ... | ... | ... | ... | ...% |
| index | ... | ... | ... | ... | ...% |
| spliced | ... | ... | ... | ... | ...% |
| align | ... | ... | ... | ... | ...% |
| compare | ... | ... | ... | ... | ...% |
| report | ... | ... | ... | ... | ...% |
| **Total** | **...** | **...** | **...** | **...** | **100%** |

## Notes

- Warm-up iteration excluded from statistics
- Index generation is a one-time cost (not timed after warm-up)
- Timings include subprocess overhead (minimap2, samtools)
```

**Important implementation details:**
- Resolve all paths relative to repo root: `Path(__file__).resolve().parent.parent`
- Suppress pipeline logging (set to WARNING level)
- Check for minimap2/samtools before starting, exit with informative error if missing
- Use `shutil.which()` for tool detection
- Handle `statistics.stdev()` with n=1 gracefully (returns 0.0)
- Include `if __name__ == "__main__":` guard
- Full type hints (mypy strict compatible)
- Follow ruff formatting conventions
- If `--steps` filter is used, only time those steps (skip others but still run them as needed -- e.g., can't skip convert if timing align)
  </action>
  <verify>
Run `python -c "import ast; ast.parse(open('scripts/benchmark.py').read())"` to verify syntax.
Run `python -m ruff check scripts/benchmark.py` to verify lint.
Run `python -m mypy scripts/benchmark.py --strict` to verify types.
  </verify>
  <done>
`scripts/benchmark.py` exists with per-step timing, configurable iterations, markdown output, and proper CLI interface. Script is syntactically valid, passes ruff lint, and has complete type annotations.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run benchmark and verify output</name>
  <files>BENCHMARK.md</files>
  <action>
1. Run the benchmark script end-to-end to verify it works:
   - `python scripts/benchmark.py -n 1 -o BENCHMARK.md` (single iteration for quick verification)
   - Verify it produces valid Markdown output with all expected sections

2. If minimap2/samtools are unavailable:
   - Verify the script detects missing tools and prints a clear error
   - Verify script structure by dry-running the argument parser: `python scripts/benchmark.py --help`

3. Verify the BENCHMARK.md output contains:
   - Version string matching `plasmicheck.version.__version__`
   - Per-step timing table with all 6 steps
   - Total row
   - Percentage column that sums to ~100%

4. If the benchmark runs successfully, also run with default settings (3 iterations) to validate statistical output:
   - `python scripts/benchmark.py`
   - Verify std dev column has non-zero values (with 3 iterations)

5. Clean up: Remove BENCHMARK.md after verification (it's gitignored and will be regenerated as needed)
  </action>
  <verify>
If tools available: `python scripts/benchmark.py -n 1` exits 0 and produces valid BENCHMARK.md.
If tools unavailable: `python scripts/benchmark.py --help` shows help text without error.
  </verify>
  <done>
Benchmark script produces valid Markdown output with per-step timing breakdown. If tools unavailable, script gives clear requirements error. TEST-02 requirement satisfied.
  </done>
</task>

</tasks>

<verification>
1. `scripts/benchmark.py` exists and is syntactically valid Python
2. Script has `-n`, `-o`, `--dataset-dir`, `--steps` CLI flags
3. Script uses `time.perf_counter()` context manager for per-step timing
4. Script uses `statistics.mean()` and `statistics.stdev()` for reporting
5. Output BENCHMARK.md has per-step timing table with mean/stdev/min/max/percent columns
6. BENCHMARK.md is gitignored (from Plan 01's .gitignore update)
7. If tools available: benchmark runs and produces valid output
</verification>

<success_criteria>
- Benchmark script times each pipeline step independently (convert, index, spliced, align, compare, report)
- Report shows mean/stdev over N iterations in Markdown table format
- Script supports custom dataset paths and iteration counts
- Warm-up iteration is excluded from statistics
- BENCHMARK.md is human-readable for PR review
- TEST-02 requirement satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/04-foundation/04-02-SUMMARY.md`
</output>
