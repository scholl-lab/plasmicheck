---
phase: 04-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/benchmark.py
autonomous: true

must_haves:
  truths:
    - "Running `python scripts/benchmark.py` times each pipeline step separately and outputs a Markdown table"
    - "Benchmark reports mean and standard deviation over 3 iterations"
    - "Benchmark times individual steps: convert, index, spliced, align, compare, report"
    - "Output BENCHMARK.md is readable and shows per-step timing breakdown"
    - "Benchmark supports both synthetic (default) and custom dataset paths"
  artifacts:
    - path: "scripts/benchmark.py"
      provides: "Standalone benchmark script with per-step timing"
      min_lines: 150
    - path: "BENCHMARK.md"
      provides: "Markdown benchmark results (generated, gitignored)"
  key_links:
    - from: "scripts/benchmark.py"
      to: "plasmicheck.scripts.*"
      via: "Imports individual step functions directly (NOT run_pipeline). Each step function is a standalone callable with simple parameters -- verified by inspecting run_pipeline.py lines 488-584 where they are called identically."
      pattern: "from plasmicheck\\.scripts\\.(convert_plasmidfile_to_fasta|create_indexes|spliced_alignment|align_reads|compare_alignments|generate_report) import"
    - from: "scripts/benchmark.py"
      to: "tests/data/synthetic/"
      via: "Default dataset path for benchmark runs"
      pattern: "tests/data/synthetic"

note: "TEST-03 (air-gapped environment test) is deferred to Phase 5 per user decision in 04-CONTEXT.md. Phase 4 delivers TEST-01 and TEST-02 only."
---

<objective>
Create a standalone benchmark script that measures per-step pipeline timing and outputs a Markdown report.

Purpose: This provides the performance comparison tool needed to validate that Phases 5-7 optimizations actually improve performance. Developers will run this before and after optimization PRs to quantify speedup per pipeline step.

Output: `scripts/benchmark.py` -- a self-contained CLI script that runs the pipeline with per-step timing and generates `BENCHMARK.md`.

Note: TEST-03 (air-gapped environment test) is deferred to Phase 5 per user decision. Phase 4 delivers TEST-01 (Plan 01) and TEST-02 (this plan) only.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-foundation/04-CONTEXT.md
@.planning/phases/04-foundation/04-RESEARCH.md

Key source files:
@plasmicheck/scripts/run_pipeline.py (pipeline orchestration -- understand step sequence)
@plasmicheck/scripts/convert_plasmidfile_to_fasta.py (convert step)
@plasmicheck/scripts/create_indexes.py (index step)
@plasmicheck/scripts/spliced_alignment.py (spliced alignment step)
@plasmicheck/scripts/align_reads.py (alignment step)
@plasmicheck/scripts/compare_alignments.py (comparison step)
@plasmicheck/scripts/generate_report.py (report generation step)
@plasmicheck/version.py (version string)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create benchmark script with per-step timing</name>
  <files>scripts/benchmark.py</files>
  <action>
Create `scripts/benchmark.py` as a standalone CLI script that times each pipeline step by calling the individual step functions directly.

**Verified: All step functions are standalone callables.** The `run_pipeline()` function in `run_pipeline.py` (lines 488-584) calls these same functions with simple arguments. The `PipelinePlan` dataclass is only used for dry-run display and skip logic -- it is NOT involved in actual execution. The benchmark script calls the same functions with the same signatures as `run_pipeline()` does.

**Exact imports and function signatures (verified from source):**

```python
from plasmicheck.scripts.convert_plasmidfile_to_fasta import convert_plasmidfile_to_fasta
# convert_plasmidfile_to_fasta(input_file: str, output_file: str, file_type: str,
#     shift_bases: int = 500, generate_shifted: bool = False, overwrite: bool = False) -> None

from plasmicheck.scripts.create_indexes import create_indexes
# create_indexes(fasta_file: str, overwrite: bool = False) -> None

from plasmicheck.scripts.spliced_alignment import (
    spliced_alignment, extract_human_reference, extract_plasmid_cdna_positions,
)
# spliced_alignment(human_index: str, plasmid_fasta: str, output_bam: str,
#     padding: int = 1000) -> list[tuple[str, int, int]]
# extract_human_reference(human_fasta: str, spanned_regions: list[tuple[str, int, int]],
#     output_fasta: str) -> None
# extract_plasmid_cdna_positions(plasmid_fasta: str, bam_file: str, output_file: str) -> None

from plasmicheck.scripts.align_reads import align_reads
# align_reads(reference_index: str, input_file: str, output_bam: str,
#     alignment_type: str, fastq2: str | None = None) -> None

from plasmicheck.scripts.compare_alignments import compare_alignments
# compare_alignments(plasmid_bam: str, human_bam: str, output_basename: str,
#     threshold: float = 0.8) -> None

from plasmicheck.scripts.generate_report import main as generate_report
# main(reads_assignment_file: str, summary_file: str, output_folder: str,
#     threshold: float = 0.8, ...) -> None
```

**CLI interface (argparse):**
- `python scripts/benchmark.py` -- Run 3 iterations on synthetic dataset, output to BENCHMARK.md
- `python scripts/benchmark.py -n 5` -- Override iteration count
- `python scripts/benchmark.py -o results.md` -- Override output file
- `python scripts/benchmark.py --dataset-dir PATH` -- Use custom dataset directory (must contain human_ref.fasta, plasmid.gb, contaminated_R1.fastq, contaminated_R2.fastq)
- `python scripts/benchmark.py --steps convert,align,compare` -- Benchmark only specific steps (comma-separated)

**Per-step timing approach:**

The benchmark replicates the same call sequence as `run_pipeline()` lines 481-584, wrapping each step with timing. The pipeline flow for a single combination is:

1. **convert**: `convert_plasmidfile_to_fasta(plasmid_gb, plasmid_fasta, "genbank")`
2. **index**: `create_indexes(human_fasta)` + `create_indexes(plasmid_fasta)`
3. **spliced**: `spliced_alignment()` returns `spanned_regions`, then `extract_human_reference()`, then `create_indexes(spliced_fasta)`, then `extract_plasmid_cdna_positions()`
4. **align**: `align_reads()` for both plasmid and human references (two calls)
5. **compare**: `compare_alignments()`
6. **report**: `generate_report()` (imported as `main` from `generate_report.py`)

Note: Step 3 (spliced) includes an internal `create_indexes(spliced_fasta)` call for the spliced reference, matching `run_pipeline.py` line 538. This must NOT be omitted.

Use a `timed_step` context manager (from RESEARCH.md Pattern 2):
```python
@contextmanager
def timed_step(name: str, timings: dict[str, float]) -> Generator[None, None, None]:
    start = time.perf_counter()
    try:
        yield
    finally:
        timings[name] = time.perf_counter() - start
```

**Path computation (mirrors run_pipeline.py lines 471-478, 482-566):**
The benchmark must compute the same intermediate file paths as `run_pipeline()`. These are deterministic string operations:
- `output_subfolder = os.path.join(output_dir, bam_basename, file_basename)` where basenames come from `sanitize_filename(os.path.splitext(os.path.basename(file))[0])`
- `plasmid_fasta = os.path.join(output_subfolder, base + ".fasta")`
- `human_index = os.path.join(os.path.dirname(human_fasta), base + ".mmi")`
- And so on for spliced_bam, spliced_fasta, plasmid_bam, spliced_human_bam, comparison_output

Import `sanitize_filename` from `plasmicheck.scripts.run_pipeline` (or `plasmicheck.scripts.utils`) to ensure path computation matches exactly.

**Iteration logic:**
1. Run one warm-up iteration (not counted) to avoid import/index-generation overhead skewing results
2. Run N iterations (default 3), collecting per-step timings for each
3. For each step, compute mean, stdev (using `statistics.mean()`, `statistics.stdev()`)
4. Also compute total pipeline time per iteration

**Important: Use tempfile.TemporaryDirectory for each iteration.**
Each iteration should run into a fresh temp output directory, then clean up. Do NOT reuse output directories because `--overwrite` behavior could skip steps.

However, index files (`.mmi`, `.fai`) should be pre-generated during warm-up and reused across iterations (they don't change). Copy the pre-generated indexes into each iteration's temp dir, or ensure indexes exist before timing starts (index step timing should reflect the steady-state, not first-run generation).

Actually, simpler approach: time the **full pipeline call** per-step by calling individual functions, but for the index step, time it once and note "index generation is a one-time cost." For subsequent iterations, skip the index step if indexes already exist (matching real-world usage).

**Markdown output format (BENCHMARK.md):**

```markdown
# PlasmiCheck Benchmark Results

**Version:** {version}
**Date:** {ISO date}
**Dataset:** {dataset name} ({read count} reads)
**Iterations:** {N} (+ 1 warm-up)
**System:** {platform.system()} {platform.machine()}

## Per-Step Timing

| Step | Mean (s) | Std Dev (s) | Min (s) | Max (s) | % of Total |
|------|----------|-------------|---------|---------|------------|
| convert | ... | ... | ... | ... | ...% |
| index | ... | ... | ... | ... | ...% |
| spliced | ... | ... | ... | ... | ...% |
| align | ... | ... | ... | ... | ...% |
| compare | ... | ... | ... | ... | ...% |
| report | ... | ... | ... | ... | ...% |
| **Total** | **...** | **...** | **...** | **...** | **100%** |

## Notes

- Warm-up iteration excluded from statistics
- Index generation is a one-time cost (not timed after warm-up)
- Timings include subprocess overhead (minimap2, samtools)
```

**Important implementation details:**
- Resolve all paths relative to repo root: `Path(__file__).resolve().parent.parent`
- Suppress pipeline logging (set to WARNING level)
- Check for minimap2/samtools before starting, exit with informative error if missing
- Use `shutil.which()` for tool detection
- Handle `statistics.stdev()` with n=1 gracefully (returns 0.0)
- Include `if __name__ == "__main__":` guard
- Full type hints (mypy strict compatible)
- Follow ruff formatting conventions
- If `--steps` filter is used, only time those steps (skip others but still run them as needed -- e.g., can't skip convert if timing align)
  </action>
  <verify>
Run `python -c "import ast; ast.parse(open('scripts/benchmark.py').read())"` to verify syntax.
Run `python -m ruff check scripts/benchmark.py` to verify lint.
Run `python -m mypy scripts/benchmark.py --strict` to verify types.
  </verify>
  <done>
`scripts/benchmark.py` exists with per-step timing using verified standalone function calls, configurable iterations, markdown output, and proper CLI interface. Script is syntactically valid, passes ruff lint, and has complete type annotations.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run benchmark and verify output</name>
  <files>BENCHMARK.md</files>
  <action>
1. Run the benchmark script end-to-end to verify it works:
   - `python scripts/benchmark.py -n 1 -o BENCHMARK.md` (single iteration for quick verification)
   - Verify it produces valid Markdown output with all expected sections

2. If minimap2/samtools are unavailable:
   - Verify the script detects missing tools and prints a clear error
   - Verify script structure by dry-running the argument parser: `python scripts/benchmark.py --help`

3. Verify the BENCHMARK.md output contains:
   - Version string matching `plasmicheck.version.__version__`
   - Per-step timing table with all 6 steps
   - Total row
   - Percentage column that sums to ~100%

4. If the benchmark runs successfully, also run with default settings (3 iterations) to validate statistical output:
   - `python scripts/benchmark.py`
   - Verify std dev column has non-zero values (with 3 iterations)

5. Clean up: Remove BENCHMARK.md after verification (it's gitignored and will be regenerated as needed)
  </action>
  <verify>
If tools available: `python scripts/benchmark.py -n 1` exits 0 and produces valid BENCHMARK.md.
If tools unavailable: `python scripts/benchmark.py --help` shows help text without error.
  </verify>
  <done>
Benchmark script produces valid Markdown output with per-step timing breakdown. If tools unavailable, script gives clear requirements error. TEST-02 requirement satisfied.
  </done>
</task>

</tasks>

<verification>
1. `scripts/benchmark.py` exists and is syntactically valid Python
2. Script has `-n`, `-o`, `--dataset-dir`, `--steps` CLI flags
3. Script uses `time.perf_counter()` context manager for per-step timing
4. Script uses `statistics.mean()` and `statistics.stdev()` for reporting
5. Output BENCHMARK.md has per-step timing table with mean/stdev/min/max/percent columns
6. BENCHMARK.md is gitignored (from Plan 01's .gitignore update)
7. If tools available: benchmark runs and produces valid output
</verification>

<success_criteria>
- Benchmark script times each pipeline step independently (convert, index, spliced, align, compare, report)
- Report shows mean/stdev over N iterations in Markdown table format
- Script supports custom dataset paths and iteration counts
- Warm-up iteration is excluded from statistics
- BENCHMARK.md is human-readable for PR review
- TEST-02 requirement satisfied
- TEST-03 deferred to Phase 5 (not in scope for this plan)
</success_criteria>

<output>
After completion, create `.planning/phases/04-foundation/04-02-SUMMARY.md`
</output>
