---
phase: 05-report-optimization
plan: 04
type: execute
wave: 3
depends_on: ["05-02", "05-03"]
files_modified:
  - tests/test_generate_report.py
  - tests/test_generate_summary_reports.py
  - tests/test_cli.py
autonomous: true

must_haves:
  truths:
    - "All existing unit tests pass after the refactoring"
    - "New CLI flags appear in help output for pipeline, report, and summary_reports"
    - "Lazy imports work correctly — module can be imported without triggering heavy imports"
    - "Regression test passes (contamination ratios and verdicts unchanged)"
    - "make ci-check passes (lint + format + typecheck + test)"
  artifacts:
    - path: "tests/test_cli.py"
      provides: "CLI tests that verify --static-report and --plotly-mode appear in help"
      contains: "static.report"
    - path: "tests/test_generate_report.py"
      provides: "Tests for refactored generate_report module"
      contains: "static_report"
    - path: "tests/test_generate_summary_reports.py"
      provides: "Tests for refactored generate_summary_reports module"
      contains: "static_report"
  key_links:
    - from: "tests/test_cli.py"
      to: "plasmicheck/cli.py"
      via: "subprocess calls testing --help output"
      pattern: "static.report"
    - from: "tests/test_generate_report.py"
      to: "plasmicheck/scripts/generate_report.py"
      via: "Direct function imports and calls"
      pattern: "generate_report"
---

<objective>
Fix any test failures from the refactoring, add test coverage for the new CLI flags, and run the full CI check and regression test to validate correctness.

Purpose: Ensures the Phase 5 refactoring didn't break anything and that all new functionality is verified. The regression test is critical — it proves contamination detection accuracy is unchanged.

Output: Updated test files, passing CI check, passing regression test.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-report-optimization/05-CONTEXT.md
@.planning/phases/05-report-optimization/05-01-SUMMARY.md
@.planning/phases/05-report-optimization/05-02-SUMMARY.md
@.planning/phases/05-report-optimization/05-03-SUMMARY.md

Source files:
@tests/test_cli.py
@tests/test_generate_report.py
@tests/test_generate_summary_reports.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix test failures and add CLI flag tests</name>
  <files>tests/test_generate_report.py, tests/test_generate_summary_reports.py, tests/test_cli.py</files>
  <action>
**A. Fix any existing test failures caused by lazy imports:**

The existing tests import functions from generate_report and generate_summary_reports. Since we moved module-level imports to function-level, the function signatures and behavior should be identical, but verify:

1. Run `make test-fast` to see what fails
2. For each failure, determine if it's caused by:
   - Import changes (function now needs pandas imported inside, but tests provide DataFrames — this should work since pd.DataFrame objects don't require the import at the caller side)
   - Signature changes (main() now has new optional params — shouldn't break existing calls)
   - Removed module-level constants or variables that tests depend on

Fix each failure. Most likely, the existing tests will still pass since:
- `downsample_data`, `extract_verdict_from_summary`, `load_data` still have the same behavior
- `apply_sorting`, `calculate_variations`, `find_tsv_files`, `_sanitize_for_excel` still have the same behavior
- Functions now import pandas internally, but they still return the same types

**B. Add CLI flag tests to test_cli.py:**

Add tests that verify the new flags appear in help output:

```python
@pytest.mark.unit
@pytest.mark.parametrize("subcommand", ["pipeline", "report", "summary_reports"])
def test_report_flags_in_help(self, subcommand: str) -> None:
    """Verify --static-report and --plotly-mode appear in subcommand help."""
    result = subprocess.run(
        [sys.executable, "-m", "plasmicheck.cli", subcommand, "--help"],
        capture_output=True,
        text=True,
        cwd=PROJECT_ROOT,
    )
    assert result.returncode == 0
    assert "--static-report" in result.stdout
    assert "--plotly-mode" in result.stdout
    assert "directory" in result.stdout  # Default value should appear
```

**C. Add lazy import verification test:**

```python
@pytest.mark.unit
def test_generate_report_lazy_imports(self) -> None:
    """Verify that importing generate_report doesn't trigger heavy imports."""
    result = subprocess.run(
        [
            sys.executable, "-c",
            "import sys; "
            "before = set(sys.modules.keys()); "
            "import plasmicheck.scripts.generate_report; "
            "after = set(sys.modules.keys()); "
            "new_modules = after - before; "
            "heavy = {'pandas', 'plotly', 'jinja2', 'kaleido'}; "
            "loaded_heavy = heavy & {m.split('.')[0] for m in new_modules}; "
            "print(','.join(sorted(loaded_heavy)) if loaded_heavy else 'NONE'); "
        ],
        capture_output=True,
        text=True,
        cwd=PROJECT_ROOT,
    )
    assert result.returncode == 0
    # pandas, plotly, jinja2, kaleido should NOT be loaded on import
    assert result.stdout.strip() == "NONE", f"Heavy modules loaded on import: {result.stdout.strip()}"
```

Add the same test for generate_summary_reports (also checking numpy, scipy, statsmodels).

**D. Run full CI check and regression test:**

1. Run `make ci-check` (lint + format + typecheck + test)
2. Fix any issues found
3. Run regression test if available: `python scripts/regression_test.py` (this requires test data — if data exists, run it; if not, note that regression testing requires the test dataset)

For the regression test: If it can't be run (no test data in the repo), document this. The regression test validates that contamination ratios and verdicts match the baseline. It was built in Phase 4 (Plan 04-01).
  </action>
  <verify>
Run `make ci-check` — all checks pass (lint, format, typecheck, test).
Run `make test-fast` — all unit tests pass, including new CLI flag tests and lazy import tests.
If regression test data exists: `python scripts/regression_test.py` passes.
  </verify>
  <done>
All existing tests pass after refactoring. New tests verify: (1) --static-report and --plotly-mode flags appear in CLI help for pipeline, report, summary_reports, (2) importing generate_report and generate_summary_reports does not trigger heavy library imports. make ci-check passes. Regression test (if runnable) confirms contamination detection accuracy unchanged.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update version and verify benchmarks</name>
  <files>plasmicheck/version.py</files>
  <action>
**A. Verify the optimization impact:**

Check the benchmark script exists and review what it measures:
```bash
ls scripts/benchmark.py
```

If benchmark can be run (requires test data), run it and document the results comparing before/after:
- Report generation time without --static-report (should be dramatically faster — no Kaleido)
- HTML file sizes with directory mode (should be ~19 KB instead of 9.6 MB)
- CLI startup time (import time for `plasmicheck.cli`)

If benchmark can't be run (no test data), document the expected improvements based on research:
- Without --static-report: eliminates 5.1s Kaleido overhead (91.7% of pipeline time)
- Directory mode HTML: ~19 KB per report (was 9.6 MB per report)
- Lazy imports: ~30-50% faster CLI startup

**B. Do NOT change the version number yet** — version bumps happen at release time, not per-phase. Leave version.py as-is.

**C. Final verification checklist:**

Run these commands and confirm all pass:
```bash
make lint        # No ruff errors
make format      # No format changes needed
make typecheck   # No mypy errors
make test-fast   # All unit tests pass
make test        # All tests including integration pass (if possible)
```

Verify requirements coverage:
- REPT-01: Default pipeline/report/summary_reports runs WITHOUT PNGs — DONE (write_image calls gated behind static_report)
- REPT-02: --static-report flag generates PNGs alongside HTML — DONE (flag on all 3 subcommands)
- REPT-03: Directory mode uses shared plotly.min.js — DONE (assets/ directory management)
- REPT-04: --plotly-mode {cdn,directory,embedded} CLI flag — DONE (on all 3 subcommands)
- REPT-05: Kaleido start_sync_server() — DONE (called once before write_image when static_report=True)
- REPT-06: Lazy imports — DONE (pandas, plotly, jinja2, numpy, scipy, statsmodels at function level)
- TEST-03: Air-gapped Docker test — DROPPED per context decisions
  </action>
  <verify>
Run `make ci-check` — everything passes.
Run `python -m plasmicheck.cli pipeline --help | grep -E "(static-report|plotly-mode)"` — both flags listed.
Run `python -c "import time; t=time.time(); import plasmicheck.scripts.generate_report; print(f'{(time.time()-t)*1000:.0f}ms')"` — should be fast (no pandas/plotly loaded).
  </verify>
  <done>
All CI checks pass. All 6 active requirements (REPT-01 through REPT-06) addressed. Lazy imports verified. Phase 5 implementation complete and ready for integration testing.
  </done>
</task>

</tasks>

<verification>
1. `make ci-check` passes (lint + format + typecheck + test)
2. All new CLI flag tests pass
3. Lazy import verification tests pass
4. No regression in contamination detection (regression test or manual verification)
5. All 6 REPT requirements addressed
</verification>

<success_criteria>
- make ci-check passes with zero failures
- --static-report and --plotly-mode flags work on all three subcommands
- No heavy imports (pandas, plotly, jinja2, kaleido, scipy, statsmodels) loaded at module import time
- Contamination detection accuracy unchanged (regression test)
- All Phase 5 requirements (REPT-01 through REPT-06) satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/05-report-optimization/05-04-SUMMARY.md`
</output>
